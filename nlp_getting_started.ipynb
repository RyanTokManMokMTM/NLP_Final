{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp-getting-started.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RyanTokManMokMTM/NLP_Final/blob/main/nlp_getting_started.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pry2qSCl01cA"
      },
      "source": [
        "# Simple Kaggle Compitition with NLP\n",
        "## Natural Language Processing with Disaster Tweets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMPilt5-3FEQ"
      },
      "source": [
        "### load package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVx7kZnrS777",
        "outputId": "6e57fc62-eebc-439d-c5a0-562d5ca399a3"
      },
      "source": [
        "#install package\n",
        "!pip install transformers"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.9.2)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw5lvO58_26T",
        "outputId": "181dc56b-0343-408c-9e98-402956597873"
      },
      "source": [
        "#package import here\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import nltk.corpus\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from transformers import BertTokenizer,BertConfig,TFBertModel\n",
        "import numpy as np\n",
        "import keras\n",
        "import tensorflow as tf\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_zFkBV8090_"
      },
      "source": [
        "### loading the data from csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVwPXWfA_xDL"
      },
      "source": [
        "def dataLoader(csvPath):\n",
        "  return pd.read_csv(csvPath,encoding=\"utf-8\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94GjPi_Io2B3"
      },
      "source": [
        "#### Define a function to remove the html url etc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GLWEQQuEyDJ"
      },
      "source": [
        "#define a clean text function\n",
        "def clean_texts(text):\n",
        "  text = text.lower()\n",
        "  #clean all not a-zA-z0-9 text\n",
        "  textRe = re.compile(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\")\n",
        "  return textRe.sub(\"\",text)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vR6RP8L7LneN"
      },
      "source": [
        "#### need to clean all stop word with cleaning words list\n",
        "#### stopword list with NL Toolkit stopword.word(english)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxTSgx_bMxbP"
      },
      "source": [
        "#clean all stop words in each sentance\n",
        "def clean_stop_word(cleanTxt):\n",
        "  return \" \".join([word for word in cleanTxt.split() if not word in stopwordsList])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqOF5iZ0TjTs"
      },
      "source": [
        "### Basic SGDClassifer(0.77)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DadXR23hDA9Z"
      },
      "source": [
        "def classifiedWithSGD(feature_x,label_y):\n",
        "  #init the pieple for sgd\n",
        "  sgd = Pipeline([\n",
        "      ('vect', CountVectorizer()),\n",
        "      ('tfidf',  TfidfTransformer()),\n",
        "      ('nb', SGDClassifier()),\n",
        "    ])\n",
        "    #fit the data to model\n",
        "  model = sgd.fit(feature_x, label_y)\n",
        "  return model"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjQSrohqQVSu"
      },
      "source": [
        "### Tokenization\n",
        "#### make the word as a vector of each sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vc9mf9FXFZnn"
      },
      "source": [
        "#init CountVectorizer  \n",
        "vectorizer = CountVectorizer(analyzer=\"word\",tokenizer=None,preprocessor=None,stop_words=None)\n",
        "def tokenizationAndFitModel(featureSet):\n",
        "  #fit the model with set\n",
        "  return vectorizer.fit_transform(featureSet).toarray()\n",
        "\n",
        "def tokenizationAndTranform(featureSet):\n",
        "  #fit the model with set\n",
        "  return vectorizer.transform(featureSet)\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQTelF7Aajd3"
      },
      "source": [
        "### Trying on Bert Model(0.57033),may need some more epoch and batch size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vfzPt-sUWgt"
      },
      "source": [
        "def generateModelArray(featureData,tokenizer,maxLength):\n",
        "  ids = []\n",
        "  masks = []\n",
        "  tokens = []\n",
        "  for text in featureData:\n",
        "    token = tokenizer.encode(text,max_length=maxLength)\n",
        "    padding_size = maxLength - len(token)\n",
        "    ids.append(token + [0]*padding_size)\n",
        "    masks.append([1]*len(token) + [0]*padding_size) #only token is 1 other padding to 0\n",
        "    tokens.append([0]*maxLength) #all 0\n",
        "  ids = np.array(ids)\n",
        "  masks = np.array(masks)\n",
        "  tokens = np.array(tokens)\n",
        "  return ids, masks,tokens"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPf3wwsSTHtu"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "MAX_SIZE = 128\n",
        "EPOCHS = 6\n",
        "LEARNING_RATE = 3e-6\n",
        "\n",
        "def createBertModel(featureData,labelData,tokenizer,config,bert_model):\n",
        "  train_ids, train_mask,train_token = generateModelArray(featureData,tokenizer,MAX_SIZE)\n",
        "\n",
        "  #train Label to np array\n",
        "  train_label = np.array(labelData)\n",
        "\n",
        "  #model argument  and 3 input\n",
        "  input_ids = keras.layers.Input(shape=(MAX_SIZE,),dtype='int32')\n",
        "  attension_mask = keras.layers.Input(shape=(MAX_SIZE,),dtype='int32')\n",
        "  token_type_ids = keras.layers.Input(shape=(MAX_SIZE,),dtype='int32')\n",
        "\n",
        "  Bertmodel  = bert_model([input_ids,attension_mask,token_type_ids])\n",
        "  lastStateOutPut = Bertmodel.last_hidden_state\n",
        "  poolerOutput =  Bertmodel.pooler_output\n",
        "\n",
        "  #model output layer\n",
        "  denseOutput = keras.layers.Dense(units=1,activation=\"sigmoid\")(poolerOutput)\n",
        "\n",
        "  #creating model\n",
        "  model = keras.models.Model(inputs=[input_ids,attension_mask,token_type_ids],outputs = denseOutput)\n",
        "\n",
        "  #model compile\n",
        "  model.compile(loss=\"binary_crossentropy\",optimizer=keras.optimizer_v2.adam.Adam(learning_rate=LEARNING_RATE),metrics=['accuracy'])\n",
        "\n",
        "  #training model\n",
        "  #data set for train and test\n",
        "  (x_train_ids,x_test_index,x_train_mask,x_test_mask,x_train_token,x_test_token,y_train,y_test)= train_test_split(train_ids,train_mask,train_token,train_label,test_size = 0.1,stratify=train_label, random_state=0)\n",
        "  \n",
        "  print(x_train_ids)\n",
        "  #early_stopping\n",
        "  training_early_stopping = keras.callbacks.EarlyStopping(patience=3,restore_best_weights=True)\n",
        "\n",
        "  #fit model\n",
        "  model.fit([x_train_ids,x_train_mask,x_train_token],y_train,epochs=EPOCHS,batch_size=BATCH_SIZE,validation_data=([x_test_index,x_test_mask,x_test_token],y_test),callbacks=[training_early_stopping])\n",
        "  return model\n"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCXCHR6TJpTx"
      },
      "source": [
        "# #predict the model\n",
        "# y_predicted = model.predict([test_ids,test_mask,test_token],batch_size=32,verbose=1)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiIs8t0wLwna"
      },
      "source": [
        "# y_predicted = y_predicted.ravel()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tqM4m8KL1yx"
      },
      "source": [
        "# y_predicted = (y_predicted >= 0.5).astype(int)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18KVv1f3E57_"
      },
      "source": [
        "# def outputSubmission(submissionFile,outputCsv):\n",
        "#   predict_submission = pd.read_csv(\"sample_submission.csv\")\n",
        "#   predict_submission[\"target\"] = predict\n",
        "#   predict_submission.to_csv(\"nlp_submission_sgd.csv\",index = False)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcV8Q9SG_ixD"
      },
      "source": [
        "# Main process is running here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrLXTeInInWl"
      },
      "source": [
        "def process_SGD(featureData,lableData,predictFeature):\n",
        "  x_train,x_test,y_train,y_test = train_test_split(featureData,lableData,random_state = 0)\n",
        "  model = classifiedWithSGD(x_train,y_train)\n",
        "\n",
        "  #predict test data set\n",
        "  predict = model.predict(x_test)\n",
        "  #show predict report\n",
        "  print(classification_report(y_test,predict))\n",
        "\n",
        "  predictResult = model.predict(predictFeature)\n",
        "  return predictResult"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1F6NAAx0JZYc"
      },
      "source": [
        "def process_RFC(featureData,lableData,predictFeature):\n",
        "  x_train,x_test,y_train,y_test = train_test_split(featureData,lableData,random_state = 0)\n",
        "  #need to tokenization word to vector\n",
        "  feature_vector = tokenizationAndFitModel(x_train)\n",
        "  \n",
        "  #create the model\n",
        "  RFC = RandomForestClassifier(n_estimators=100)\n",
        "\n",
        "  #fit moodel with tokenization vector\n",
        "  RFC = RFC.fit(feature_vector,y_train)\n",
        "  test_vector = tokenizationAndTranform(x_test)\n",
        "  # #predict \n",
        "  RFCPredict = RFC.predict(test_vector)\n",
        "\n",
        "  #print predict report\n",
        "  print(classification_report(y_test,RFCPredict))\n",
        "\n",
        "  #predict the result with testSet\n",
        "  test_setVector = tokenizationAndTranform(testSetFeature)\n",
        "  RFCPredict = RFC.predict(test_setVector)\n",
        "  return RFCPredict"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyKVJ5gELgXT"
      },
      "source": [
        "def process_bert(featureData, labelData,predictFeature):\n",
        "  tokenizer = BertTokenizer.from_pretrained(\"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\")\n",
        "  config = BertConfig.from_pretrained(\"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json\")\n",
        "  bert_model = TFBertModel.from_pretrained(\"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-tf_model.h5\",config=config)\n",
        "  model = createBertModel(featureData, labelData,tokenizer,config,bert_model)\n",
        "\n",
        "  #predict with model\n",
        "  predict_ids,predict_masks,predict_tokens = generateModelArray(predictFeature,tokenizer,MAX_SIZE)\n",
        "  preicted_result = model.predict([predict_ids,predict_masks,predict_tokens],batch_size=64,verbose=1)\n",
        "  return preicted_result"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSxgWwcA_nC0",
        "outputId": "f75d76bd-bf72-41f2-e04b-5e8782e0b2ee"
      },
      "source": [
        "#main\n",
        "if  __name__ == '__main__':\n",
        "  #login to google drive\n",
        "  drive.mount('/content/gdrive')\n",
        "\n",
        "  train_data = dataLoader(\"./gdrive/MyDrive/NLP_dataset/train.csv\")\n",
        "  test_data = dataLoader(\"./gdrive/MyDrive/NLP_dataset/test.csv\")\n",
        "  submission_data = dataLoader(\"./gdrive/MyDrive/NLP_dataset/sample_submission.csv\")\n",
        "\n",
        "  #drop out the data we don't need it\n",
        "  train_data.drop(['id', 'keyword', 'location'],axis=1)\n",
        "\n",
        "  #split feature and label\n",
        "  features = train_data[\"text\"].tolist()\n",
        "  label = train_data[\"target\"].tolist()\n",
        "  test_features = test_data[\"text\"].tolist()\n",
        "\n",
        "  #all clean training data set(stop words not clean yet)\n",
        "  clean_feature_data = [] #for training\n",
        "  clean_feature_test_data = [] #for testing\n",
        "  for txt in features:\n",
        "    clean_feature_data.append(clean_texts(txt))\n",
        "\n",
        "  for txt in test_features:\n",
        "    clean_feature_test_data.append(clean_texts(txt))\n",
        "  \n",
        "  #get stop word english list\n",
        "  stopwordsList = stopwords.words(\"english\")\n",
        "\n",
        "  #remove all stopword from sentance\n",
        "  all_clean_text = []\n",
        "  all_clean_testSet_text = []\n",
        "  for sentance in clean_feature_data:\n",
        "    all_clean_text.append(clean_stop_word(sentance))\n",
        "  for sentance in clean_feature_test_data:\n",
        "    all_clean_testSet_text.append(clean_stop_word(sentance))\n",
        "\n",
        "  #ged_result = process_SGD(all_clean_text,label,all_clean_testSet_text)\n",
        "  #rfc_result = process_RFC(all_clean_text,label,all_clean_testSet_text)\n",
        "  bert_reuslt = process_bert(all_clean_text,label,all_clean_testSet_text)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1641: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
            "  FutureWarning,\n",
            "Some layers from the model checkpoint at https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-tf_model.h5 were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-tf_model.h5.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "[[  101 28997  4971 ...     0     0     0]\n",
            " [  101  5175 11629 ...     0     0     0]\n",
            " [  101  5926 20645 ...     0     0     0]\n",
            " ...\n",
            " [  101  4584  5343 ...     0     0     0]\n",
            " [  101  4575  2987 ...     0     0     0]\n",
            " [  101  2630  4330 ...     0     0     0]]\n",
            "Epoch 1/6\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.5240 - accuracy: 0.7514WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "215/215 [==============================] - 184s 786ms/step - loss: 0.5240 - accuracy: 0.7514 - val_loss: 0.4316 - val_accuracy: 0.8045\n",
            "Epoch 2/6\n",
            "215/215 [==============================] - 170s 793ms/step - loss: 0.4172 - accuracy: 0.8194 - val_loss: 0.4018 - val_accuracy: 0.8360\n",
            "Epoch 3/6\n",
            "215/215 [==============================] - 171s 794ms/step - loss: 0.3784 - accuracy: 0.8406 - val_loss: 0.3909 - val_accuracy: 0.8373\n",
            "Epoch 4/6\n",
            "215/215 [==============================] - 171s 794ms/step - loss: 0.3537 - accuracy: 0.8552 - val_loss: 0.3924 - val_accuracy: 0.8386\n",
            "Epoch 5/6\n",
            "215/215 [==============================] - 170s 793ms/step - loss: 0.3286 - accuracy: 0.8678 - val_loss: 0.3980 - val_accuracy: 0.8465\n",
            "Epoch 6/6\n",
            "215/215 [==============================] - 171s 793ms/step - loss: 0.3042 - accuracy: 0.8774 - val_loss: 0.4369 - val_accuracy: 0.8268\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "51/51 [==============================] - 29s 515ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYgnD41QnAW8"
      },
      "source": [
        "# bert_reuslt = bert_reuslt.ravel()"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQKMTqemtrlL",
        "outputId": "287181b2-671c-4f74-868a-d5ce16bebaf0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# bert_reuslt =(bert_reuslt>=0.5).astype(int)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, ..., 1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xz0NkE0Gfuw"
      },
      "source": [
        "  # predict_submission = pd.read_csv(\"./gdrive/MyDrive/NLP_dataset/sample_submission.csv\")\n",
        "  # submission_data[\"target\"] = bert_reuslt\n",
        "  # predict_submission.to_csv(\"nlp_submission_bert.csv\",index = False)"
      ],
      "execution_count": 69,
      "outputs": []
    }
  ]
}