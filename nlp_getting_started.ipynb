{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp-getting-started.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RyanTokManMokMTM/NLP_Final/blob/main/nlp_getting_started.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pry2qSCl01cA"
      },
      "source": [
        "# Simple Kaggle Compitition with NLP\n",
        "## Natural Language Processing with Disaster Tweets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMPilt5-3FEQ"
      },
      "source": [
        "### load package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVx7kZnrS777",
        "outputId": "b5527f9e-7870-4624-facf-54cf05fc0351"
      },
      "source": [
        "#install package\n",
        "!pip install transformers"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.9.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw5lvO58_26T",
        "outputId": "54dd6e7f-122d-47d5-b8eb-d7af2b66742e"
      },
      "source": [
        "#package import here\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import nltk.corpus\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from transformers import BertTokenizer,BertConfig,TFBertModel\n",
        "import numpy as np\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold,cross_val_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_zFkBV8090_"
      },
      "source": [
        "### loading the data from csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVwPXWfA_xDL"
      },
      "source": [
        "def dataLoader(csvPath):\n",
        "  return pd.read_csv(csvPath,encoding=\"utf-8\")"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94GjPi_Io2B3"
      },
      "source": [
        "#### Define a function to remove the html url etc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GLWEQQuEyDJ"
      },
      "source": [
        "#define a clean text function\n",
        "def clean_texts(text):\n",
        "  text = text.lower()\n",
        "  #clean all not a-zA-z0-9 text\n",
        "  textRe = re.compile(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\")\n",
        "  return textRe.sub(\"\",text)"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vR6RP8L7LneN"
      },
      "source": [
        "#### need to clean all stop word with cleaning words list\n",
        "#### stopword list with NL Toolkit stopword.word(english)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxTSgx_bMxbP"
      },
      "source": [
        "#clean all stop words in each sentance\n",
        "def clean_stop_word(cleanTxt):\n",
        "  return \" \".join([word for word in cleanTxt.split() if not word in stopwordsList])"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqOF5iZ0TjTs"
      },
      "source": [
        "### Basic SGDClassifer\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DadXR23hDA9Z"
      },
      "source": [
        "def classifiedWithSGD(feature_x,label_y):\n",
        "  #init the pieple for sgd\n",
        "  sgd = Pipeline([ # a Classifier combine CountVectorizer() and TfidfTransformer()\n",
        "      ('vect', CountVectorizer()), \n",
        "      ('tfidf',  TfidfTransformer()),\n",
        "      ('nb', SGDClassifier()),\n",
        "    ])\n",
        "    #fit the data to model\n",
        "  model = sgd.fit(feature_x, label_y)\n",
        "  return model"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjQSrohqQVSu"
      },
      "source": [
        "### Tokenization\n",
        "#### make the word as a vector of each sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vc9mf9FXFZnn"
      },
      "source": [
        "#init CountVectorizer  \n",
        "vectorizer = CountVectorizer(analyzer=\"word\",tokenizer=None,preprocessor=None,stop_words=None)\n",
        "def tokenizationAndFitModel(featureSet):\n",
        "  #fit the model with set\n",
        "  return vectorizer.fit_transform(featureSet).toarray()\n",
        "\n",
        "def tokenizationAndTranform(featureSet):\n",
        "  #fit the model with set\n",
        "  return vectorizer.transform(featureSet)\n"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQTelF7Aajd3"
      },
      "source": [
        "### Bert Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vfzPt-sUWgt"
      },
      "source": [
        "def generateModelArray(featureData,tokenizer,maxLength):\n",
        "  ids = []\n",
        "  masks = []\n",
        "  tokens = []\n",
        "  for text in featureData:\n",
        "    token = tokenizer.encode(text,max_length=maxLength)\n",
        "    padding_size = maxLength - len(token)\n",
        "    ids.append(token + [0]*padding_size)\n",
        "    masks.append([1]*len(token) + [0]*padding_size) #only token is 1 other padding to 0\n",
        "    tokens.append([0]*maxLength) #all 0\n",
        "  ids = np.array(ids)\n",
        "  masks = np.array(masks)\n",
        "  tokens = np.array(tokens)\n",
        "  return ids, masks,tokens"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPf3wwsSTHtu"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "MAX_SIZE = 128\n",
        "EPOCHS = 6\n",
        "LEARNING_RATE = 3e-6\n",
        "\n",
        "def createBertModel(featureData,labelData,tokenizer,config,bert_model):\n",
        "  train_ids, train_mask,train_token = generateModelArray(featureData,tokenizer,MAX_SIZE)\n",
        "\n",
        "  #train Label to np array\n",
        "  train_label = np.array(labelData)\n",
        "\n",
        "  #model argument  and 3 input\n",
        "  input_ids = keras.layers.Input(shape=(MAX_SIZE,),dtype='int32')\n",
        "  attension_mask = keras.layers.Input(shape=(MAX_SIZE,),dtype='int32')\n",
        "  token_type_ids = keras.layers.Input(shape=(MAX_SIZE,),dtype='int32')\n",
        "\n",
        "  Bertmodel  = bert_model([input_ids,attension_mask,token_type_ids])\n",
        "  lastStateOutPut = Bertmodel.last_hidden_state\n",
        "  poolerOutput =  Bertmodel.pooler_output\n",
        "\n",
        "  #model output layer\n",
        "  denseOutput = keras.layers.Dense(units=1,activation=\"sigmoid\")(poolerOutput)\n",
        "\n",
        "  #creating model\n",
        "  model = keras.models.Model(inputs=[input_ids,attension_mask,token_type_ids],outputs = denseOutput)\n",
        "\n",
        "  #model compile\n",
        "  model.compile(loss=\"binary_crossentropy\",optimizer=keras.optimizer_v2.adam.Adam(learning_rate=LEARNING_RATE),metrics=['accuracy'])\n",
        "\n",
        "  #training model\n",
        "  #data set for train and test\n",
        "  (x_train_ids,x_test_index,x_train_mask,x_test_mask,x_train_token,x_test_token,y_train,y_test)= train_test_split(train_ids,train_mask,train_token,train_label,test_size = 0.1,stratify=train_label, random_state=0)\n",
        "  \n",
        "  print(x_train_ids)\n",
        "  #early_stopping\n",
        "  training_early_stopping = keras.callbacks.EarlyStopping(patience=3,restore_best_weights=True)\n",
        "\n",
        "  #fit model\n",
        "  model.fit([x_train_ids,x_train_mask,x_train_token],y_train,epochs=EPOCHS,batch_size=BATCH_SIZE,validation_data=([x_test_index,x_test_mask,x_test_token],y_test),callbacks=[training_early_stopping])\n",
        "  return model\n"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCXCHR6TJpTx"
      },
      "source": [
        "# #predict the model\n",
        "# y_predicted = model.predict([test_ids,test_mask,test_token],batch_size=32,verbose=1)"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiIs8t0wLwna"
      },
      "source": [
        "# y_predicted = y_predicted.ravel()"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tqM4m8KL1yx"
      },
      "source": [
        "# y_predicted = (y_predicted >= 0.5).astype(int)"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18KVv1f3E57_"
      },
      "source": [
        "def outputSubmission(submissionFile,outputCsv,result):\n",
        "  predict_submission = pd.read_csv(submissionFile)\n",
        "  predict_submission[\"target\"] = result\n",
        "  predict_submission.to_csv(outputCsv,index = False)"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcV8Q9SG_ixD"
      },
      "source": [
        "# Main process is running here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_prrkCSnwz8Z"
      },
      "source": [
        "def process_Logistic(featureData,lableData,predictFeature): #0.79037\n",
        "  x_train,x_test,y_train,y_test = train_test_split(featureData,lableData,random_state = 0)\n",
        "  feature_vector = tokenizationAndFitModel(x_train)\n",
        "  logReg = LogisticRegression(C=1)\n",
        "  logReg.fit(feature_vector,y_train)\n",
        "\n",
        "  test_vector = tokenizationAndTranform(x_test)\n",
        "  #predict test data set\n",
        "  predict = logReg.predict(test_vector)\n",
        "  #show predict report\n",
        "  print(classification_report(y_test,predict))\n",
        "\n",
        "  #predict the result\n",
        "  predict_vector = tokenizationAndTranform(predictFeature)\n",
        "  predictResult = logReg.predict(predict_vector)\n",
        "  return predictResult"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeCKqkjK34iV"
      },
      "source": [
        "def process_NavieBayes(featureData,lableData,predictFeature):#not using piple 0.78087 /pipeline 0.78884\n",
        "  x_train,x_test,y_train,y_test = train_test_split(featureData,lableData,random_state = 0)\n",
        "  feature_vector = tokenizationAndFitModel(x_train)\n",
        "\n",
        "  NB = MultinomialNB()\n",
        "  NB = NB.fit(feature_vector,y_train)\n",
        "  NB_pipe = Pipeline([\n",
        "      (\"vec\",CountVectorizer()),\n",
        "      (\"tfidf\",TfidfTransformer()),\n",
        "      (\"NB\",NB)          \n",
        "  ])\n",
        "\n",
        "  NB_pipe = NB_pipe.fit(x_train,y_train)\n",
        "  predict = NB_pipe.predict(x_test)\n",
        "  #show predict report\n",
        "  print(classification_report(y_test,predict))\n",
        "\n",
        "  predictResult = NB_pipe.predict(predictFeature)\n",
        "  return predictResult"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrLXTeInInWl"
      },
      "source": [
        "def process_SGD(featureData,lableData,predictFeature):#0.78\n",
        "  x_train,x_test,y_train,y_test = train_test_split(featureData,lableData,random_state = 0)\n",
        "  model = classifiedWithSGD(x_train,y_train)\n",
        "\n",
        "  #predict test data set\n",
        "  predict = model.predict(x_test)\n",
        "  #show predict report\n",
        "  print(classification_report(y_test,predict))\n",
        "\n",
        "  predictResult = model.predict(predictFeature)\n",
        "  return predictResult"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1F6NAAx0JZYc"
      },
      "source": [
        "def process_RFC(featureData,lableData,predictFeature):#0.79129\n",
        "  x_train,x_test,y_train,y_test = train_test_split(featureData,lableData,random_state = 0)\n",
        "  #need to tokenization word to vector\n",
        "  feature_vector = tokenizationAndFitModel(x_train)\n",
        "  \n",
        "  #create the model\n",
        "  RFC = RandomForestClassifier(n_estimators=100)\n",
        "\n",
        "  #fit moodel with tokenization vector\n",
        "  RFC = RFC.fit(feature_vector,y_train)\n",
        "  test_vector = tokenizationAndTranform(x_test)\n",
        "  # #predict \n",
        "  RFCPredict = RFC.predict(test_vector)\n",
        "\n",
        "  #print predict report\n",
        "  print(classification_report(y_test,RFCPredict))\n",
        "\n",
        "  #predict the result with testSet\n",
        "  test_setVector = tokenizationAndTranform(predictFeature)\n",
        "  RFCPredict = RFC.predict(test_setVector)\n",
        "  return RFCPredict"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyKVJ5gELgXT"
      },
      "source": [
        "def process_bert(featureData, labelData,predictFeature):#0.57033\n",
        "  tokenizer = BertTokenizer.from_pretrained(\"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\")\n",
        "  config = BertConfig.from_pretrained(\"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json\")\n",
        "  bert_model = TFBertModel.from_pretrained(\"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-tf_model.h5\",config=config)\n",
        "  model = createBertModel(featureData, labelData,tokenizer,config,bert_model)\n",
        "\n",
        "  #predict with model\n",
        "  predict_ids,predict_masks,predict_tokens = generateModelArray(predictFeature,tokenizer,MAX_SIZE)\n",
        "  preicted_result = model.predict([predict_ids,predict_masks,predict_tokens],batch_size=64,verbose=1)\n",
        "  return preicted_result"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSxgWwcA_nC0",
        "outputId": "ef20852d-4c5e-49e3-e9ae-06e3578e2a1a"
      },
      "source": [
        "#main\n",
        "if  __name__ == '__main__':\n",
        "  #login to google drive\n",
        "  drive.mount('/content/gdrive')\n",
        "\n",
        "  train_data = dataLoader(\"./gdrive/MyDrive/NLP_dataset/train.csv\")\n",
        "  test_data = dataLoader(\"./gdrive/MyDrive/NLP_dataset/test.csv\")\n",
        "\n",
        "  #drop out the data we don't need it\n",
        "  train_data.drop(['id', 'keyword', 'location'],axis=1)\n",
        "\n",
        "  #split feature and label\n",
        "  features = train_data[\"text\"].tolist()\n",
        "  label = train_data[\"target\"].tolist()\n",
        "  test_features = test_data[\"text\"].tolist()\n",
        "\n",
        "  #all clean training data set(stop words not clean yet)\n",
        "  clean_feature_data = [] #for training\n",
        "  clean_feature_test_data = [] #for testing\n",
        "  for txt in features:\n",
        "    clean_feature_data.append(clean_texts(txt))\n",
        "\n",
        "  for txt in test_features:\n",
        "    clean_feature_test_data.append(clean_texts(txt))\n",
        "  \n",
        "  #get stop word english list\n",
        "  stopwordsList = stopwords.words(\"english\")\n",
        "\n",
        "  #remove all stopword from sentance\n",
        "  all_clean_text = []\n",
        "  all_clean_testSet_text = []\n",
        "  for sentance in clean_feature_data:\n",
        "    all_clean_text.append(clean_stop_word(sentance))\n",
        "  for sentance in clean_feature_test_data:\n",
        "    all_clean_testSet_text.append(clean_stop_word(sentance))\n",
        "\n",
        "  #log_result = process_Logistic(all_clean_text,label,all_clean_testSet_text)\n",
        "  #nb_result  = process_NavieBayes(all_clean_text,label,all_clean_testSet_text)\n",
        "  #sgd_result = process_SGD(all_clean_text,label,all_clean_testSet_text)\n",
        "  #rfc_result = process_RFC(all_clean_text,label,all_clean_testSet_text)\n",
        "  #bert_reuslt = process_bert(all_clean_text,label,all_clean_testSet_text)"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.86      0.83      1107\n",
            "           1       0.79      0.72      0.75       797\n",
            "\n",
            "    accuracy                           0.80      1904\n",
            "   macro avg       0.80      0.79      0.79      1904\n",
            "weighted avg       0.80      0.80      0.80      1904\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yi3xxJvQ_ban"
      },
      "source": [
        "outputSubmission(\"./gdrive/MyDrive/NLP_dataset/sample_submission.csv\",\"nlp_submission_nb.csv\",nb_result)"
      ],
      "execution_count": 171,
      "outputs": []
    }
  ]
}